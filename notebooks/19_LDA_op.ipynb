{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "\n",
    "In this notebook, we're going to learn about an unsupervised learning approach from NLP called Topic Modeling. This is a broad category, and we're going to learn about its most popular implementation, Latent Dirichlet Allocation.\n",
    "\n",
    "Here's some intuition about the goals of Topic Modeling:\n",
    "  * Similar in spirit to cluster techniques - find things with are similar \n",
    "  * Our datapoints are documents and our features are word counts\n",
    "  * Documents that are about a similar subject probably use a lot of the same words with high probability\n",
    "  * Thus, the clusters of word counts can be thought of as simply \"topics\" (eg. Sports, Politics, Business)\n",
    "  * This is unsupervised learning, so we want to extract these hidden topics from our corpus (we have no labels)\n",
    "  * In LDA, there can be some overlap of the topics within a single document. A news article might use words and concepts from both economics and politics.\n",
    "  \n",
    "We're going to use the *gensim* package to do LDA.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read yelp.csv into a DataFrame\n",
    "url = '../data/yelp.csv'\n",
    "yelp = pd.read_csv(url)\n",
    "X = yelp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past, we've used CountVectorizer to do things like remove stop words, select the top features, and construct a document term matrix. We're going to do some of those things by hand, and let gensim take care of some others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove common words and tokenize\n",
    "\n",
    "# lets steal the stopwords from CountVectorizer. \n",
    "# So we'll initalialize a CountVectorizer() but we won't use it, we just want its word list.\n",
    "\n",
    "stoplist = set(CountVectorizer(stop_words='english').get_stop_words() )\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in list(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count up the frequency of each word\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "     for token in text:\n",
    "         frequency[token] += 1\n",
    "        \n",
    "        \n",
    "\n",
    "# remove words that only occur a small number of times\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to create our bag-of-words representation of the data. Previously, we used CountVectorizer to generate a document-term-matrix. Gensim has similar functionality for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(6838 unique tokens: [u'yellow', u\"friend's\", u'hanging', u'deli', u'regional']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(115, 1), (793, 1), (3677, 1), (3864, 1)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this objects stores everything we need for our bag of words representation\n",
    "dictionary.doc2bow(\"golf car window waiter\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a corpus in the bag of words representation by transforming each of our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 3), (12, 1), (15, 1), (16, 4), (19, 1), (25, 4), (36, 3), (37, 1), (44, 1), (50, 2), (54, 4), (56, 1), (59, 5), (60, 1), (63, 1), (65, 5), (83, 1), (88, 9), (91, 2), (93, 1), (96, 1), (105, 2), (108, 3), (113, 1), (127, 1), (131, 3), (140, 4), (148, 2), (149, 1), (154, 1), (157, 1), (159, 1), (161, 2), (165, 1), (168, 7), (174, 5), (175, 1), (180, 4), (181, 1), (182, 2), (185, 1), (186, 1), (189, 2), (192, 4), (193, 2), (196, 1), (251, 1), (264, 2), (282, 1), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1), (289, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), (295, 1), (296, 1), (297, 1), (298, 1), (299, 1), (300, 1), (301, 1), (302, 1), (303, 2), (304, 1), (305, 1), (306, 1), (307, 1), (308, 1), (309, 1), (310, 1), (311, 1), (312, 1), (313, 1), (314, 1), (315, 1), (316, 1), (317, 1), (318, 1), (319, 1), (320, 1), (321, 1), (322, 1), (323, 1), (324, 2), (325, 1), (326, 1), (327, 1), (328, 1), (329, 1), (330, 1), (331, 1), (332, 1), (333, 2), (334, 1), (335, 1), (336, 1), (337, 1), (338, 1), (339, 1), (340, 3), (341, 2), (342, 1), (343, 2), (344, 1), (345, 1), (346, 1), (347, 1), (348, 1), (349, 1), (350, 4), (351, 1), (352, 1), (353, 1), (354, 2), (355, 1), (356, 1), (357, 1), (358, 1), (359, 1), (360, 1), (361, 1), (362, 1), (363, 1), (364, 1), (365, 1), (366, 1), (367, 2), (368, 1), (369, 1), (370, 1), (371, 1), (372, 1), (373, 2), (374, 1), (375, 1), (376, 1), (377, 1), (378, 1), (379, 1), (380, 1), (381, 1), (382, 1), (383, 1), (384, 1), (385, 1), (386, 1), (387, 1), (388, 2), (389, 1), (390, 1), (391, 1), (392, 1), (393, 1), (394, 1), (395, 1), (396, 3), (397, 1), (398, 1), (399, 4), (400, 1), (401, 1), (402, 1), (403, 1), (404, 1), (405, 1), (406, 1), (407, 1), (408, 1), (409, 1), (410, 1), (411, 1), (412, 1), (413, 1), (414, 1), (415, 1), (416, 1), (417, 1), (418, 1), (419, 1), (420, 1), (421, 1), (422, 1), (423, 1), (424, 1), (425, 1), (426, 1), (427, 1), (428, 1), (429, 1), (430, 1), (431, 1), (432, 1), (433, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our corpus, we will fit an LDA model to this data. We can pick any number of topics we like and see how the result turns out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=10, alpha = 'auto')\n",
    "# warning, this takes a minute or two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.011*- + 0.009*food + 0.009*ordered + 0.009*good + 0.008*like + 0.008*place + 0.007*just + 0.007*great + 0.006*best + 0.006*really',\n",
       " u\"0.014*good + 0.010*food + 0.010*it's + 0.009*like + 0.008*chicken + 0.008*just + 0.007*place + 0.006*got + 0.006*little + 0.006*burger\",\n",
       " u\"0.011*just + 0.011*place + 0.009*good + 0.008*like + 0.007*really + 0.007*food + 0.007*i'm + 0.005*it's + 0.005*didn't + 0.005*little\",\n",
       " u\"0.015*it's + 0.010*just + 0.008*like + 0.007*really + 0.006*place + 0.006*don't + 0.005*- + 0.005*good + 0.005*food + 0.005*i've\",\n",
       " u\"0.014*like + 0.012*just + 0.010*it's + 0.010*- + 0.008*place + 0.007*really + 0.006*don't + 0.005*good + 0.005*food + 0.005*got\",\n",
       " u\"0.009*place + 0.009*just + 0.007*love + 0.007*like + 0.007*time + 0.006*good + 0.006*- + 0.006*food + 0.006*great + 0.005*it's\",\n",
       " u\"0.014*place + 0.012*like + 0.011*food + 0.008*good + 0.008*just + 0.008*it's + 0.008*really + 0.006*pizza + 0.006*- + 0.006*pretty\",\n",
       " u'0.014*food + 0.011*service + 0.010*dog + 0.006*asked + 0.006*good + 0.005*delivery + 0.005*went + 0.005*pizza + 0.005*excellent + 0.005*customer',\n",
       " u'0.030*great + 0.021*food + 0.018*place + 0.009*service + 0.009*good + 0.007*like + 0.005*definitely + 0.005*best + 0.005*lunch + 0.004*menu',\n",
       " u\"0.015*great + 0.011*good + 0.010*place + 0.009*like + 0.008*nice + 0.008*just + 0.008*really + 0.006*wine + 0.006*it's + 0.006*don't\"]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0.014065597961374307, u'place'),\n",
       "   (0.013558365075836501, u'good'),\n",
       "   (0.012024852163405949, u'food'),\n",
       "   (0.0077697351437719222, u'just'),\n",
       "   (0.0067911028195645996, u'service'),\n",
       "   (0.0062420825318874424, u'really'),\n",
       "   (0.0058014426375006057, u'great'),\n",
       "   (0.0055872844700345049, u'time'),\n",
       "   (0.0055568370110874454, u\"it's\"),\n",
       "   (0.0053055146042074791, u'got'),\n",
       "   (0.0051860177915481614, u'like'),\n",
       "   (0.0050400587262547265, u\"i'm\"),\n",
       "   (0.0047866217048370529, u'-'),\n",
       "   (0.0044233059799882006, u\"i've\"),\n",
       "   (0.0043957643274345629, u'little'),\n",
       "   (0.0040343576117017887, u\"didn't\"),\n",
       "   (0.0037519270388395288, u\"don't\"),\n",
       "   (0.003584143032704142, u'came'),\n",
       "   (0.0035284232220913485, u'went'),\n",
       "   (0.0031748211735253455, u'good.')],\n",
       "  -504.48052087638246),\n",
       " ([(0.0098665676284805902, u'like'),\n",
       "   (0.0084474962467171455, u'food'),\n",
       "   (0.0073160061650787119, u'place'),\n",
       "   (0.0072213481376723959, u'good'),\n",
       "   (0.0059832165880357707, u'just'),\n",
       "   (0.0046442391853328364, u'came'),\n",
       "   (0.0045145202749965734, u'great'),\n",
       "   (0.0043233613798171304, u'people'),\n",
       "   (0.0042998674717852426, u\"i'm\"),\n",
       "   (0.0042103581649984524, u'chicken'),\n",
       "   (0.0041644062774728588, u\"it's\"),\n",
       "   (0.0041281074046679907, u'-'),\n",
       "   (0.0040961878485373483, u'really'),\n",
       "   (0.0037544240693151115, u'restaurant'),\n",
       "   (0.0035022025409039213, u'make'),\n",
       "   (0.0031398156553384799, u'service'),\n",
       "   (0.0029572982404938849, u'nice'),\n",
       "   (0.0026955720532139838, u'got'),\n",
       "   (0.0026872546747463834, u'ordered'),\n",
       "   (0.0025994403926685472, u'time')],\n",
       "  -528.27472908838524),\n",
       " ([(0.011174237269966265, u'like'),\n",
       "   (0.0092800178204374521, u'place'),\n",
       "   (0.0090636904966423654, u'just'),\n",
       "   (0.0086905504691901104, u'great'),\n",
       "   (0.0085931281289718092, u'-'),\n",
       "   (0.0076288834181821143, u\"it's\"),\n",
       "   (0.0075752714122702793, u'really'),\n",
       "   (0.0061094411570834983, u\"don't\"),\n",
       "   (0.0056012200230095583, u'food'),\n",
       "   (0.005364822799865397, u'good'),\n",
       "   (0.0046389080538475711, u'pizza'),\n",
       "   (0.0043511180964724491, u\"i'm\"),\n",
       "   (0.0041691809350885862, u'chicken'),\n",
       "   (0.0041561324072632942, u'pretty'),\n",
       "   (0.0038588987454209159, u'little'),\n",
       "   (0.0037862153401238543, u'ordered'),\n",
       "   (0.0035884151439837014, u'love'),\n",
       "   (0.0035022984182598153, u\"i've\"),\n",
       "   (0.0034156284900768675, u'nice'),\n",
       "   (0.0032716878316029446, u'time')],\n",
       "  -531.19172405526683),\n",
       " ([(0.0095945463835642748, u\"it's\"),\n",
       "   (0.0095180280652771877, u'like'),\n",
       "   (0.0092904573376667663, u'place'),\n",
       "   (0.0071861323816650351, u\"i've\"),\n",
       "   (0.0061808754516386706, u'just'),\n",
       "   (0.0061684485878034179, u'love'),\n",
       "   (0.0061288582409372875, u'good'),\n",
       "   (0.0054197483961669606, u'great'),\n",
       "   (0.0053946329095007359, u'really'),\n",
       "   (0.0049721102685626462, u\"don't\"),\n",
       "   (0.0042141788069069543, u'staff'),\n",
       "   (0.0041404769446023813, u'best'),\n",
       "   (0.0036041559249943844, u\"i'm\"),\n",
       "   (0.003561091636549182, u'store'),\n",
       "   (0.0033667214767227082, u'nice'),\n",
       "   (0.0032428336901974479, u'make'),\n",
       "   (0.0032159645595086321, u'-'),\n",
       "   (0.0031580231602649023, u'time'),\n",
       "   (0.0031462749617900223, u'way'),\n",
       "   (0.0030860613642530077, u'little')],\n",
       "  -550.15452978113626),\n",
       " ([(0.0096243010688444128, u'place'),\n",
       "   (0.008942871708295521, u'food'),\n",
       "   (0.008312057400850649, u'breakfast'),\n",
       "   (0.0073543409468950337, u\"it's\"),\n",
       "   (0.0069326698817726443, u'like'),\n",
       "   (0.0066078480496706161, u'great'),\n",
       "   (0.006117787977225903, u'just'),\n",
       "   (0.0057618598023221395, u'coffee'),\n",
       "   (0.0055306946093109655, u'love'),\n",
       "   (0.0055201398632621027, u'best'),\n",
       "   (0.0049462146561611037, u'good'),\n",
       "   (0.0047563526983674378, u'lunch'),\n",
       "   (0.0046824825446249874, u'&'),\n",
       "   (0.0040585969731192035, u'order'),\n",
       "   (0.0037927917168521503, u'time'),\n",
       "   (0.0029496496395493227, u'little'),\n",
       "   (0.0029090116607387549, u'really'),\n",
       "   (0.0028946043827484187, u'service'),\n",
       "   (0.0028310914060785388, u'hot'),\n",
       "   (0.0026934235899288375, u'fresh')],\n",
       "  -555.42243824761113),\n",
       " ([(0.010044602172564926, u'just'),\n",
       "   (0.0090607685501962708, u'like'),\n",
       "   (0.0082263875587120922, u'-'),\n",
       "   (0.0074122650907966934, u'great'),\n",
       "   (0.0064212962694427094, u'place'),\n",
       "   (0.0058285481116336427, u'good'),\n",
       "   (0.0055216012238187388, u'food'),\n",
       "   (0.0046786808550303444, u\"it's\"),\n",
       "   (0.0041820039245718732, u'little'),\n",
       "   (0.0038932778619695549, u'time'),\n",
       "   (0.0036681964882376264, u'fries'),\n",
       "   (0.0034176162334762345, u'love'),\n",
       "   (0.0033791283911689847, u'burger'),\n",
       "   (0.0031819500195934109, u'really'),\n",
       "   (0.0030298705668733695, u'nice'),\n",
       "   (0.0029349369000349081, u'try'),\n",
       "   (0.0029296679442105357, u'went'),\n",
       "   (0.0029243522780172432, u'did'),\n",
       "   (0.0029148787159567272, u'best'),\n",
       "   (0.0028846344378792737, u\"don't\")],\n",
       "  -561.69426976066973),\n",
       " ([(0.0073777548783936966, u'good'),\n",
       "   (0.0070291581605116619, u'place'),\n",
       "   (0.0070063175253218899, u'just'),\n",
       "   (0.0064083686683634247, u'like'),\n",
       "   (0.0062217563150527011, u'food'),\n",
       "   (0.0062026817891076983, u'great'),\n",
       "   (0.0052097137455565353, u\"it's\"),\n",
       "   (0.0047912023012685271, u'.'),\n",
       "   (0.0042338151317778006, u'time'),\n",
       "   (0.0040403151015486289, u'pretty'),\n",
       "   (0.0040398626633781007, u'little'),\n",
       "   (0.0035630280519305868, u'coffee'),\n",
       "   (0.0034730658579898237, u'come'),\n",
       "   (0.0033643389846526829, u'new'),\n",
       "   (0.0031598442782308335, u\"i'm\"),\n",
       "   (0.0029374789366344452, u'best'),\n",
       "   (0.0028504117534703974, u'service'),\n",
       "   (0.0027847549415887002, u'nice'),\n",
       "   (0.0027242225912832052, u'really'),\n",
       "   (0.0026875725865380768, u\"don't\")],\n",
       "  -585.51030776252162),\n",
       " ([(0.0068924901094023485, u\"don't\"),\n",
       "   (0.0068074470177950028, u'like'),\n",
       "   (0.0061597565486320562, u'time'),\n",
       "   (0.0059401319692635804, u\"it's\"),\n",
       "   (0.0053459217991352207, u'just'),\n",
       "   (0.0048272435071480156, u'really'),\n",
       "   (0.0047194233368108894, u'going'),\n",
       "   (0.0045646015012137725, u'place'),\n",
       "   (0.0039876872067740095, u'good'),\n",
       "   (0.003648030425587714, u'love'),\n",
       "   (0.0035995657740756689, u'make'),\n",
       "   (0.0035980831700332034, u'bar'),\n",
       "   (0.0035592126684585506, u'beer'),\n",
       "   (0.0034909106278251632, u'people'),\n",
       "   (0.0033757484474212823, u'pretty'),\n",
       "   (0.003357748073727468, u'free'),\n",
       "   (0.0032499349810221762, u\"i'm\"),\n",
       "   (0.0030945480122829046, u'think'),\n",
       "   (0.0029142873902742983, u'food'),\n",
       "   (0.00287228531058913, u'course')],\n",
       "  -589.6952927568417),\n",
       " ([(0.013448852604096808, u'food'),\n",
       "   (0.0069051056118211015, u'good'),\n",
       "   (0.006440760811881184, u'service'),\n",
       "   (0.0059495406241876839, u'place'),\n",
       "   (0.0058744493065217467, u'time'),\n",
       "   (0.0057723588983435688, u'ordered'),\n",
       "   (0.0055906017777611026, u'great'),\n",
       "   (0.0053693602635732944, u'just'),\n",
       "   (0.0047544937544996037, u'like'),\n",
       "   (0.0045266773700590743, u'chicken'),\n",
       "   (0.0041508010078627694, u'restaurant'),\n",
       "   (0.004126639281720107, u'little'),\n",
       "   (0.0040945488793245636, u'really'),\n",
       "   (0.0038573484358869628, u'shrimp'),\n",
       "   (0.0037228557464577751, u'menu'),\n",
       "   (0.0035183598391614391, u'try'),\n",
       "   (0.0035162690660507372, u'came'),\n",
       "   (0.0034736924569142621, u'dish'),\n",
       "   (0.0031166776709452958, u'beef'),\n",
       "   (0.0028479463460813273, u'dishes')],\n",
       "  -607.53373649668015),\n",
       " ([(0.0082055830528134591, u'great'),\n",
       "   (0.0065787018250189466, u'good'),\n",
       "   (0.0063870958647501246, u'happy'),\n",
       "   (0.006311548950242489, u'just'),\n",
       "   (0.0054418937326053953, u'hour'),\n",
       "   (0.0047253555492149562, u'place'),\n",
       "   (0.0047162150909181707, u'like'),\n",
       "   (0.0046005046105967425, u'food'),\n",
       "   (0.0044084878540444532, u'really'),\n",
       "   (0.0036080234526900319, u\"it's\"),\n",
       "   (0.0035997426372020263, u'time'),\n",
       "   (0.0035335749855319662, u'definitely'),\n",
       "   (0.003500531290033632, u'nice'),\n",
       "   (0.0032990049584327536, u\"i'm\"),\n",
       "   (0.0029336577627183044, u'did'),\n",
       "   (0.0029116616183028389, u'&'),\n",
       "   (0.0029040059614346518, u'-'),\n",
       "   (0.0027783394013975488, u'car'),\n",
       "   (0.0027068495398965137, u\"didn't\"),\n",
       "   (0.0027008786036316701, u'night')],\n",
       "  -615.53299943760464)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.top_topics(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "  * Try this again with only 2 topics. How do the topics seem?\n",
    "  * Try this again without removing stop words. How do the topics seem?\n",
    "  * From what you've learned about LDA, do you think this method would work better with documents the size of tweets or the size of long news articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
